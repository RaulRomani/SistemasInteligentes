{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "#to convert html to text\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#dealing with operating system , like reading file\n",
    "import os\n",
    "\n",
    "#from nltk.corpus import stopwords\n",
    "from __future__ import division\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a tokenized corpus for a word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raul/anaconda2/envs/gl-env/lib/python2.7/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('shuffled_movie_data.csv')\n",
    "\n",
    "#SOME CLEANING\n",
    "def remove_punctuation(text):    \n",
    "    text = BeautifulSoup(text).get_text().encode('ascii','ignore')\n",
    "    return text.translate(None, string.punctuation) \n",
    "\n",
    "# Remove punctuation.\n",
    "df['review_clean'] = df['review'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the corpus\n",
    "corpus = df['review_clean'].values.tolist()\n",
    "\n",
    "# Tokenize corpus\n",
    "tok_corpus = [nltk.word_tokenize(sent.decode('utf-8')) for sent in corpus ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK so I really like Kris Kristofferson and his usual easy going delivery of lines in his movies Age has helped him with his soft spoken low energy style and he will steal a scene effortlessly But Disappearance is his misstep Holy Moly this was a bad movie I must give kudos to the cinematography and and the actors including Kris for trying their darndest to make sense from this goofy confusing story None of it made sense and Kris probably didnt understand it either and he was just going through the motions hoping someone would come up to him and tell him what it was all about I dont care that everyone on this movie was doing out of love for the project or some such nonsense Ive seen low budget movies that had a plot for goodness sake This had none zilcho nada zippo empty of reason a complete waste of good talent scenery and celluloid I rented this piece of garbage for a buck and I want my money back I want my 2 hours back I invested on this Grade F waste of my time Dont watch this movie or waste 1 minute of your valuable time while passing through a room where its playing or even open up the case that is holding the DVD Believe me youll thank me for the advice\n",
      "[u'OK', u'so', u'I', u'really', u'like', u'Kris', u'Kristofferson', u'and', u'his', u'usual', u'easy', u'going', u'delivery', u'of', u'lines', u'in', u'his', u'movies', u'Age', u'has', u'helped', u'him', u'with', u'his', u'soft', u'spoken', u'low', u'energy', u'style', u'and', u'he', u'will', u'steal', u'a', u'scene', u'effortlessly', u'But', u'Disappearance', u'is', u'his', u'misstep', u'Holy', u'Moly', u'this', u'was', u'a', u'bad', u'movie', u'I', u'must', u'give', u'kudos', u'to', u'the', u'cinematography', u'and', u'and', u'the', u'actors', u'including', u'Kris', u'for', u'trying', u'their', u'darndest', u'to', u'make', u'sense', u'from', u'this', u'goofy', u'confusing', u'story', u'None', u'of', u'it', u'made', u'sense', u'and', u'Kris', u'probably', u'didnt', u'understand', u'it', u'either', u'and', u'he', u'was', u'just', u'going', u'through', u'the', u'motions', u'hoping', u'someone', u'would', u'come', u'up', u'to', u'him', u'and', u'tell', u'him', u'what', u'it', u'was', u'all', u'about', u'I', u'dont', u'care', u'that', u'everyone', u'on', u'this', u'movie', u'was', u'doing', u'out', u'of', u'love', u'for', u'the', u'project', u'or', u'some', u'such', u'nonsense', u'Ive', u'seen', u'low', u'budget', u'movies', u'that', u'had', u'a', u'plot', u'for', u'goodness', u'sake', u'This', u'had', u'none', u'zilcho', u'nada', u'zippo', u'empty', u'of', u'reason', u'a', u'complete', u'waste', u'of', u'good', u'talent', u'scenery', u'and', u'celluloid', u'I', u'rented', u'this', u'piece', u'of', u'garbage', u'for', u'a', u'buck', u'and', u'I', u'want', u'my', u'money', u'back', u'I', u'want', u'my', u'2', u'hours', u'back', u'I', u'invested', u'on', u'this', u'Grade', u'F', u'waste', u'of', u'my', u'time', u'Dont', u'watch', u'this', u'movie', u'or', u'waste', u'1', u'minute', u'of', u'your', u'valuable', u'time', u'while', u'passing', u'through', u'a', u'room', u'where', u'its', u'playing', u'or', u'even', u'open', u'up', u'the', u'case', u'that', u'is', u'holding', u'the', u'DVD', u'Believe', u'me', u'youll', u'thank', u'me', u'for', u'the', u'advice']\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "print (corpus[1])\n",
    "print (tok_corpus[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a word2vec model (using gensim library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "# Dimensionality of the resulting word vectors.\n",
    "#more dimensions, more computationally expensive to train\n",
    "#but also more accurate\n",
    "#more dimensions = more generalized\n",
    "num_features = 400\n",
    "\n",
    "# Minimum word count threshold.\n",
    "min_word_count = 12      #  ignore all words with total frequency lower than this.\n",
    "\n",
    "# Number of threads to run in parallel.\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "# Context window length.\n",
    "#window is the maximum distance between the current and predicted word within a sentence.\n",
    "context_size = 7\n",
    "\n",
    "# Downsample setting for frequent words.\n",
    "#rate 0 and 1e-5 \n",
    "#how often to use\n",
    "downsampling = 1e-3\n",
    "\n",
    "# Seed for the RNG, to make the results reproducible.\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews2vec_model = gensim.models.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cannot sort vocabulary after model weights already initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-16fcb0b4eb72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreviews2vec_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtok_corpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/raul/anaconda2/envs/gl-env/lib/python2.7/site-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36mbuild_vocab\u001b[1;34m(self, sentences, keep_raw_vocab, trim_rule, progress_per, update)\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprogress_per\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# initial survey\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# trim by min_count & precalculate downsampling\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinalize_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# build tables & arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mscan_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/raul/anaconda2/envs/gl-env/lib/python2.7/site-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36mfinalize_vocab\u001b[1;34m(self, update)\u001b[0m\n\u001b[0;32m    734\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msorted_vocab\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m             \u001b[1;31m# add info about each word's Huffman encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/raul/anaconda2/envs/gl-env/lib/python2.7/site-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36msort_vocab\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    757\u001b[0m         \u001b[1;34m\"\"\"Sort the vocabulary so the most frequent words have the lowest indexes.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cannot sort vocabulary after model weights already initialized.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cannot sort vocabulary after model weights already initialized."
     ]
    }
   ],
   "source": [
    "reviews2vec_model.build_vocab(tok_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Word2Vec vocabulary length:', 30092)\n"
     ]
    }
   ],
   "source": [
    "print(\"Word2Vec vocabulary length:\", len(reviews2vec_model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "print (reviews2vec_model.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41659898"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews2vec_model.train(tok_corpus, total_examples=reviews2vec_model.corpus_count, epochs=reviews2vec_model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'terrible', 0.8175176382064819),\n",
       " (u'atrocious', 0.7761964797973633),\n",
       " (u'horrendous', 0.7675600051879883),\n",
       " (u'horrible', 0.7653225660324097),\n",
       " (u'dreadful', 0.7593313455581665),\n",
       " (u'horrid', 0.7593281865119934),\n",
       " (u'lousy', 0.7511627674102783),\n",
       " (u'appalling', 0.7324734926223755),\n",
       " (u'Awful', 0.7061058282852173),\n",
       " (u'abysmal', 0.705527663230896)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "reviews2vec_model.most_similar('awful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save and load the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save model\n",
    "if not os.path.exists(\"trained\"):\n",
    "    os.makedirs(\"trained\")\n",
    "reviews2vec_model.save(os.path.join(\"trained\", \"reviews2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you already have trained a model, you can start executing the lines from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load model\n",
    "reviews2vec_model = gensim.models.Word2Vec.load(os.path.join(\"trained\", \"reviews2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split data \n",
    "#90% - train set,\n",
    "#10% - validation set,\n",
    "\n",
    "train_data, validation_data = np.split(df.sample(frac=1), [int(.9*len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 3)\n",
      "(5000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>Were I not with friends, and so cheap, I would...</td>\n",
       "      <td>0</td>\n",
       "      <td>Were I not with friends and so cheap I would h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3614</th>\n",
       "      <td>MPAA Rating PG-13&lt;br /&gt;&lt;br /&gt;My Rating: 10 and...</td>\n",
       "      <td>1</td>\n",
       "      <td>MPAA Rating PG13My Rating 10 and upMy  Rating ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42657</th>\n",
       "      <td>I am surprised that so many comments about thi...</td>\n",
       "      <td>0</td>\n",
       "      <td>I am surprised that so many comments about thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8420</th>\n",
       "      <td>I also saw The Last Stop at the Moving Picture...</td>\n",
       "      <td>1</td>\n",
       "      <td>I also saw The Last Stop at the Moving Picture...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44511</th>\n",
       "      <td>I clicked onto the Encore Mystery channel to w...</td>\n",
       "      <td>0</td>\n",
       "      <td>I clicked onto the Encore Mystery channel to w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment  \\\n",
       "9991   Were I not with friends, and so cheap, I would...          0   \n",
       "3614   MPAA Rating PG-13<br /><br />My Rating: 10 and...          1   \n",
       "42657  I am surprised that so many comments about thi...          0   \n",
       "8420   I also saw The Last Stop at the Moving Picture...          1   \n",
       "44511  I clicked onto the Encore Mystery channel to w...          0   \n",
       "\n",
       "                                            review_clean  \n",
       "9991   Were I not with friends and so cheap I would h...  \n",
       "3614   MPAA Rating PG13My Rating 10 and upMy  Rating ...  \n",
       "42657  I am surprised that so many comments about thi...  \n",
       "8420   I also saw The Last Stop at the Moving Picture...  \n",
       "44511  I clicked onto the Encore Mystery channel to w...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print train_data.shape\n",
    "print validation_data.shape\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 1: define new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features:\n",
      "400\n",
      "\n",
      "features for one review:\n",
      "[ 0.02156176  0.14134539  0.08880428  0.05256522 -0.14419097  0.20558562\n",
      " -0.16841365  0.20405753  0.06917345  0.1581469 ]\n"
     ]
    }
   ],
   "source": [
    "def getFeaturesFromReview( review, reviews2vec_model ):\n",
    "    \n",
    "    tok_review = nltk.word_tokenize( review )\n",
    "    features = np.zeros( reviews2vec_model.vector_size )\n",
    "    \n",
    "    #for each word of the review, consult that word in the w2v_model, then add to the review2vec\n",
    "    for word in tok_review:\n",
    "        if word in reviews2vec_model:                        \n",
    "            features +=  np.array( reviews2vec_model[word] )\n",
    "        \n",
    "    features /=  len( tok_review )  #normalize review\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "#test function\n",
    "print \"number of features:\"\n",
    "print( len(getFeaturesFromReview( df['review_clean'][1], reviews2vec_model )) )\n",
    "print \"\\nfeatures for one review:\"\n",
    "print( getFeaturesFromReview( df['review_clean'][1], reviews2vec_model )[0:10] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "cadena = \"featurue_\" + str(1)\n",
    "print len(train_data)\n",
    "print len(validation_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getFeatureMatrix(data_frame, reviews2vec_model):\n",
    "    feature_matrix   = np.zeros( ( len(data_frame) , reviews2vec_model.vector_size ) )\n",
    "\n",
    "    reviews = data_frame['review_clean'].values\n",
    "    for i in range(len(data_frame)):\n",
    "        feature_matrix[i] = getFeaturesFromReview(reviews[i], reviews2vec_model)\n",
    "    \n",
    "    return feature_matrix\n",
    "\n",
    "\n",
    "feature_matrix_train = getFeatureMatrix(train_data, reviews2vec_model)\n",
    "sentiment_matrix_train = train_data['sentiment'].values\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observations x features\n",
      "(45000, 400)\n",
      "[ 0.0616405   0.12841867  0.10909981  0.06343006 -0.10798091  0.18266215\n",
      " -0.17067333  0.15067648  0.02028038  0.17144921  0.19480498  0.06771892\n",
      " -0.06612274 -0.16720054 -0.00759485 -0.04147195  0.06266785 -0.09646977\n",
      " -0.02220956 -0.12962679 -0.06481798  0.09190133  0.01777032  0.01433943\n",
      " -0.11267799  0.07886701  0.08487322 -0.09199788  0.09229028 -0.15855721\n",
      "  0.02422404  0.10432643 -0.02578599  0.02992763  0.03873751 -0.06719865\n",
      " -0.01111012  0.03457657 -0.21048486 -0.06384608 -0.04244143  0.0778416\n",
      "  0.00200225 -0.05728661  0.16785249 -0.07047689 -0.00187041  0.114737\n",
      "  0.07178662 -0.01052806  0.16197565 -0.10114054  0.0865088  -0.10153684\n",
      " -0.04348475  0.18130604  0.1562086   0.16619596 -0.06502336  0.04683475\n",
      "  0.00874115  0.11823475  0.02112846  0.03213234  0.10520442 -0.11300841\n",
      " -0.11355467  0.0761749  -0.06140949  0.06587857 -0.00774678  0.26487703\n",
      " -0.04014797  0.1716031   0.05950917 -0.00485175  0.00258972 -0.02121212\n",
      "  0.10969694 -0.09273628  0.08816319  0.15326491  0.06410572 -0.11195741\n",
      "  0.10717697  0.14710821  0.00038573  0.0597043   0.07278508 -0.00134582\n",
      " -0.12315107 -0.03809074 -0.15600721  0.06382787  0.05818827  0.07714399\n",
      "  0.03976502 -0.18425125 -0.07031209  0.11025051  0.09532615 -0.02793117\n",
      "  0.02476132  0.07873078 -0.07393675 -0.07877488  0.17442409  0.07992368\n",
      "  0.01932679  0.01660025 -0.12410305  0.01914821  0.01092013 -0.12244636\n",
      " -0.01344754  0.13569659 -0.01292064 -0.10928125  0.04619261 -0.02652335\n",
      "  0.07152788  0.03726815  0.00831475  0.09092892 -0.17424787 -0.1581134\n",
      " -0.02245319  0.02864261 -0.04369106  0.00640891  0.124748    0.06034999\n",
      "  0.07390908  0.17589416 -0.10781223 -0.00543844  0.06530727  0.01872873\n",
      " -0.05022353  0.11749734  0.05477649  0.10491746 -0.00810994 -0.06665602\n",
      "  0.12034144 -0.0515023   0.06378     0.02552659  0.02077606 -0.11723939\n",
      " -0.0708347  -0.01186848 -0.17849816  0.02312196  0.00581817 -0.04690131\n",
      "  0.15069372 -0.03124971 -0.04416017 -0.04620933  0.01807157  0.10185286\n",
      "  0.05485259 -0.14568144 -0.1673058   0.0282665   0.15152671 -0.10830626\n",
      "  0.00541458  0.00826121  0.10860182 -0.13322833  0.05093687  0.04927076\n",
      " -0.0476578   0.05080891  0.13815126  0.04280103 -0.1815461   0.01862132\n",
      "  0.02331871 -0.04133175 -0.10489255  0.08982078 -0.09223008 -0.0840046\n",
      "  0.07045006 -0.04361276 -0.04993913  0.10722402 -0.06591853  0.09372781\n",
      "  0.09531191 -0.10946748  0.17369051 -0.00754011  0.06506372  0.00653866\n",
      " -0.19504392 -0.1229292   0.05251135  0.04491723  0.07673886  0.026647\n",
      "  0.09370624  0.21525578  0.0611443  -0.05340047 -0.01104599  0.02180096\n",
      " -0.22420832 -0.10968282  0.1136885   0.12247241  0.10951015 -0.16741953\n",
      "  0.09832846 -0.0662472  -0.02759473 -0.10409747 -0.0472432  -0.16789346\n",
      " -0.00868601  0.07747578 -0.14539387 -0.09465365  0.08295582 -0.06792921\n",
      "  0.12980353  0.05703193  0.03670099 -0.14638675  0.05966164 -0.10359419\n",
      "  0.00820754  0.00775318 -0.0256234   0.03691842 -0.07182011 -0.05179419\n",
      "  0.1169992  -0.0095959   0.05482287 -0.01464588 -0.02594099 -0.08903921\n",
      " -0.04153837  0.14657039 -0.13006971 -0.07380736  0.01207386  0.06740595\n",
      " -0.02703531  0.08716001  0.08114631 -0.00058541  0.1083165  -0.06477933\n",
      "  0.00033014 -0.01296287  0.04585348  0.07841155 -0.04115197  0.01442879\n",
      " -0.02205706  0.10432895  0.13325319 -0.15535452  0.01013913 -0.16450891\n",
      "  0.06323924  0.01266693  0.07107293 -0.08483988  0.10023322 -0.10754554\n",
      " -0.01046578  0.09677536  0.01836982 -0.18506635  0.02520241 -0.00218293\n",
      "  0.02040628  0.01971761  0.02018015  0.01680712  0.04897473  0.0617478\n",
      "  0.11017116 -0.10196582 -0.0847616   0.11589297  0.00126055 -0.06995629\n",
      " -0.03827248 -0.08131489  0.0925342  -0.06161987 -0.09099478  0.12453213\n",
      "  0.03491918 -0.01030116  0.05503764  0.08696812  0.07074677  0.08668247\n",
      " -0.01747789  0.06834443 -0.13975484  0.03943331 -0.03013589 -0.0862097\n",
      "  0.15470288 -0.03202992 -0.046953    0.1085624   0.13220769 -0.15172285\n",
      "  0.03148248 -0.07823932  0.03304194 -0.10324818 -0.03715367  0.01917978\n",
      "  0.08431514 -0.08964091  0.01919599 -0.12839124 -0.03636424 -0.01896175\n",
      "  0.0509188   0.0723521  -0.02275029  0.02637834  0.00097019  0.00815474\n",
      "  0.09987231  0.13106481 -0.01850007  0.08068125  0.00643837  0.02949674\n",
      "  0.11722808  0.009434    0.006858   -0.05737767  0.02756137 -0.11321971\n",
      " -0.10112047 -0.01578266  0.02214205  0.03074489 -0.04891638  0.04892664\n",
      " -0.08938255 -0.02229664 -0.02429096 -0.01967158 -0.0203618   0.06004313\n",
      "  0.01572459 -0.00141906  0.06818574 -0.10360701 -0.14987621  0.14548098\n",
      " -0.04219261 -0.03000388 -0.10533148  0.02112016  0.19669725  0.07637383\n",
      " -0.17104744 -0.03095633  0.04498487  0.02956277  0.0635945   0.07322811\n",
      " -0.05401591 -0.06817852 -0.05583473 -0.01773368  0.03957793 -0.07142169\n",
      " -0.04531393  0.21881053  0.1724556  -0.02104705 -0.01529794 -0.07232033\n",
      "  0.04002414  0.11164384 -0.04827303 -0.12129163  0.08843165  0.18438736\n",
      " -0.01663674 -0.13473041  0.02789462  0.14121783]\n"
     ]
    }
   ],
   "source": [
    "print \"observations x features\"\n",
    "print feature_matrix_train.shape\n",
    "print feature_matrix_train[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observations x features\n",
      "(5000, 400)\n",
      "[  1.01931002e-02   7.73581650e-02   8.47699401e-02   5.49387786e-02\n",
      "  -1.07852798e-01   1.56354084e-01  -1.49090488e-01   1.57247423e-01\n",
      "   3.79283632e-02   1.33099873e-01   1.26898337e-01   6.56414283e-02\n",
      "  -5.73762854e-02  -8.58825491e-02  -3.65617679e-02  -7.05558592e-02\n",
      "   6.23098300e-02  -7.30688211e-02  -5.72165984e-02  -5.54534699e-02\n",
      "  -7.32261716e-02   5.17013417e-02  -1.31778424e-02  -3.71448484e-02\n",
      "  -9.05236041e-02   8.78914643e-02   3.03008168e-02  -1.60629378e-02\n",
      "   6.86010827e-02  -1.27573358e-01   3.91782224e-02   1.07194706e-01\n",
      "  -7.90004530e-02   1.40557669e-02   3.99485763e-02  -3.28727923e-02\n",
      "   4.79369051e-03   4.19837957e-02  -1.54084905e-01  -6.65031556e-02\n",
      "  -7.02485679e-02   8.90962751e-02   4.23207929e-02  -6.83618520e-02\n",
      "   1.51499883e-01  -6.29515199e-02  -2.38640371e-02   8.57518937e-02\n",
      "   5.61278428e-02  -1.49770049e-02   9.57415580e-02  -8.97936206e-02\n",
      "   8.37957609e-02  -4.00362706e-02  -5.54858274e-02   1.10331281e-01\n",
      "   9.93051651e-02   1.33998163e-01  -2.06891216e-02   4.28027109e-02\n",
      "  -2.84227834e-02   6.70916980e-02   2.15985202e-02   5.66568703e-02\n",
      "   2.43302812e-02  -4.83957669e-02  -7.39927255e-02   6.30312924e-02\n",
      "  -3.37714630e-02   6.27261593e-02  -5.84866898e-02   2.00421938e-01\n",
      "  -1.48209825e-03   1.16628438e-01  -4.18753006e-02  -3.25853754e-02\n",
      "   2.76573909e-03  -2.89559011e-02   8.98031252e-02  -5.26652038e-02\n",
      "   8.90112286e-02   1.44245404e-01   6.83875119e-03  -4.73503500e-02\n",
      "   7.28947342e-02   1.50703436e-01  -7.84646758e-03   5.55789852e-02\n",
      "   7.66708620e-02  -1.07976558e-02  -1.35891982e-01  -2.59827423e-02\n",
      "  -1.10431619e-01   3.94297914e-02   5.51599862e-02   2.74716411e-02\n",
      "   1.32235308e-02  -1.55676665e-01  -2.74949766e-02   8.22847024e-02\n",
      "   1.08406535e-01  -2.18969576e-02  -4.66589840e-02   3.73402243e-02\n",
      "  -7.90273339e-02  -4.56699135e-02   8.02345578e-02   1.06850562e-01\n",
      "   3.76509859e-02   1.31397492e-02  -7.95678384e-02   2.28726496e-02\n",
      "  -1.30756661e-02  -8.08046955e-02   3.72087725e-02   7.15751119e-02\n",
      "  -6.46494567e-02  -8.62690122e-02   5.86921645e-02  -3.65642442e-02\n",
      "  -1.15018083e-03  -5.08110111e-04  -2.36350991e-02   6.07482418e-02\n",
      "  -1.32360291e-01  -1.87707089e-01   2.50431947e-02  -1.65233863e-02\n",
      "  -5.74790050e-02  -2.83743281e-04   7.16239970e-02  -1.24191411e-03\n",
      "   4.59886022e-02   9.41162201e-02  -8.60763041e-02  -3.21422020e-02\n",
      "   1.59137782e-02  -4.47061813e-02  -4.85496055e-02   8.26505925e-02\n",
      "   4.26124244e-02   8.60155628e-02   1.33026721e-02  -7.37233637e-02\n",
      "   9.45043514e-02  -8.82308834e-02   7.36592211e-02   7.03704083e-02\n",
      "   1.71346872e-02  -4.96003470e-02  -2.79322419e-02   7.24203821e-03\n",
      "  -1.35885730e-01  -5.32936732e-02   1.25025094e-02  -2.66503770e-02\n",
      "   1.16646207e-01  -4.20702940e-02  -2.48425252e-02  -3.12616370e-02\n",
      "   6.72540248e-03   1.04819990e-01  -1.27056824e-02  -8.45085603e-02\n",
      "  -1.17530862e-01  -2.50559234e-03   9.98690644e-02  -3.46037090e-02\n",
      "   7.47397157e-03   1.61839032e-02   6.47029131e-02  -1.13290540e-01\n",
      "   3.42903928e-02   6.43819868e-02  -4.20473314e-02   5.69913990e-02\n",
      "   1.24241359e-01   9.63361309e-03  -1.18223576e-01  -9.37443347e-03\n",
      "  -1.81942888e-02  -3.82517638e-03  -5.08718286e-02   5.00133613e-02\n",
      "  -4.80649125e-02  -3.03534592e-02   9.33585818e-02  -3.28258866e-02\n",
      "  -3.63603145e-02   8.66288714e-02  -9.06272001e-02   7.90487261e-02\n",
      "   1.16008402e-01  -9.29793413e-02   1.66904145e-01  -8.39240898e-03\n",
      "   2.72926597e-02  -4.15217302e-02  -1.83008992e-01  -9.50484622e-02\n",
      "   3.69233114e-02   4.48053190e-02   7.76746344e-02   2.80799831e-02\n",
      "   9.30055469e-02   1.26803618e-01   7.90657067e-02  -1.20778172e-01\n",
      "  -1.02400207e-02   7.08012714e-02  -1.83440034e-01  -9.68456848e-02\n",
      "   1.50226971e-01   1.46405430e-01   1.22004716e-01  -1.23284371e-01\n",
      "   8.30375091e-02  -2.56838931e-02  -2.70063823e-02  -1.06756240e-01\n",
      "  -1.52994769e-02  -1.92583465e-01  -8.23408827e-03   4.46058010e-02\n",
      "  -9.90329746e-02  -4.18832839e-02   9.99963516e-02  -8.34222082e-02\n",
      "   1.03891583e-01   4.80687139e-02  -1.16178958e-02  -1.03429651e-01\n",
      "   8.37000074e-02  -9.77242405e-02   3.67533388e-02  -1.41368254e-02\n",
      "  -4.21341371e-02  -4.49219406e-02  -7.59932007e-02  -2.78460302e-02\n",
      "   1.23607985e-01  -1.72659429e-02   8.58358271e-02  -1.47082537e-02\n",
      "  -8.78219319e-02   1.54719641e-04  -9.48940641e-02   1.39292826e-01\n",
      "  -1.49701687e-01  -1.07316659e-01  -3.35726113e-02   6.26501897e-02\n",
      "   4.51569636e-02   2.47019203e-02   1.23524593e-01  -3.02842912e-02\n",
      "   1.07013019e-01   6.68675794e-03  -3.34462795e-02   1.58745266e-02\n",
      "   3.72214228e-02   9.32325486e-02  -1.14977763e-01   1.02075639e-02\n",
      "  -3.60530463e-03   7.20283375e-02   9.54879156e-02  -1.05639628e-01\n",
      "  -1.29261643e-03  -1.75507426e-01   7.09223786e-03  -7.99007867e-03\n",
      "   1.15139545e-01  -7.17233716e-02   1.12468083e-01  -1.18564390e-01\n",
      "   3.59924239e-03   1.38639149e-01   1.55210227e-02  -1.47757495e-01\n",
      "   3.00328771e-02  -6.31684765e-02   1.99513290e-02  -2.28452435e-02\n",
      "  -1.38731136e-02  -4.02774083e-02   5.21624637e-02   5.73403634e-02\n",
      "   4.76608503e-02  -9.79029961e-02  -7.00763222e-02   1.27015604e-01\n",
      "  -6.23626846e-03  -4.09936367e-02  -4.32991892e-02  -1.22955708e-01\n",
      "   9.75717739e-02  -3.36046454e-03  -4.97052005e-02   2.35437415e-02\n",
      "   5.24789475e-02  -3.68608400e-02   7.65537217e-02   4.09096860e-02\n",
      "   4.60568147e-02   1.12364898e-01   7.03692301e-02   6.92565243e-02\n",
      "  -6.76580188e-02   2.13329565e-02  -4.46320994e-02  -2.54385853e-02\n",
      "   1.59887259e-01  -5.31524618e-02  -4.97021418e-03   7.67716568e-02\n",
      "   1.01690398e-01  -1.50009206e-01   2.20852298e-02  -2.92011388e-02\n",
      "   4.96219878e-02  -9.97559375e-02  -1.60999868e-02   1.16615893e-01\n",
      "   4.45752558e-02  -8.37212350e-02   3.06324288e-02  -1.18638378e-01\n",
      "   1.68136571e-02  -2.61529540e-02   3.55110356e-02   4.64658445e-02\n",
      "  -1.84866699e-02   3.39091797e-02  -2.72185505e-02  -1.89517569e-02\n",
      "   8.17005677e-02   1.14503585e-01  -1.70290219e-02   1.15144316e-01\n",
      "  -7.80204036e-02   3.94633790e-02   9.45837424e-02  -2.75053092e-02\n",
      "  -7.70202867e-03  -6.02093659e-02   1.12297014e-02  -6.69239845e-02\n",
      "  -7.98435594e-02  -1.44149344e-02  -8.12198721e-03   1.64271251e-03\n",
      "  -2.33556026e-02   3.27158269e-02  -4.37488716e-02   9.99767688e-03\n",
      "  -2.04865821e-02  -3.74401753e-02  -3.79120751e-02   4.42633782e-02\n",
      "   2.00821103e-02   5.50173504e-02   4.28319934e-02  -8.86604117e-02\n",
      "  -1.33777896e-01   7.03406834e-02  -1.64857483e-02  -4.99223208e-02\n",
      "  -6.34800132e-02   1.29520913e-02   1.67650518e-01   1.63016815e-02\n",
      "  -1.20609668e-01  -6.16263938e-02   6.73754281e-02   6.63714716e-02\n",
      "   1.57843282e-01   5.78020378e-02  -8.79490354e-02  -1.11264930e-01\n",
      "  -5.46713672e-02  -7.16694558e-03  -3.46630681e-02  -3.97137207e-02\n",
      "  -7.53847412e-02   1.85477431e-01   1.87565202e-01  -9.68688336e-04\n",
      "  -2.27600625e-02  -2.38034031e-02   2.46672754e-02   6.86812094e-02\n",
      "  -7.85838459e-02  -8.50089911e-02   6.47946107e-02   1.63156823e-01\n",
      "  -5.48722043e-02  -1.36302602e-01  -1.03740478e-02   8.60285339e-02]\n"
     ]
    }
   ],
   "source": [
    "feature_matrix_valid = getFeatureMatrix(validation_data, reviews2vec_model)\n",
    "sentiment_matrix_valid = validation_data['sentiment'].values\n",
    "\n",
    "\n",
    "print \"observations x features\"\n",
    "print feature_matrix_valid.shape\n",
    "print feature_matrix_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "print sentiment_matrix_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "sentiment_matrix_train = np.reshape(sentiment_matrix_train, (-1, 1))\n",
    "sentiment_matrix_train = sentiment_matrix_train.T\n",
    "feature_matrix_train = feature_matrix_train.T\n",
    "\n",
    "sentiment_matrix_valid = np.reshape(sentiment_matrix_valid, (-1, 1))\n",
    "sentiment_matrix_valid = sentiment_matrix_valid.T\n",
    "feature_matrix_valid = feature_matrix_valid.T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features x observations\n",
      "(400, 5000)\n",
      "(400, 45000)\n",
      "(1, 5000)\n",
      "(1, 45000)\n"
     ]
    }
   ],
   "source": [
    "print \"features x observations\"\n",
    "print feature_matrix_valid.shape\n",
    "print feature_matrix_train.shape\n",
    "\n",
    "print sentiment_matrix_valid.shape\n",
    "print sentiment_matrix_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 2: implement a MaxEnt classifier, using regularization, according this https://web.stanford.edu/~jurafsky/slp3/7.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the sigmoid function I get better results than using the probability function of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Helper method for neural network classifier with gradient descent \n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1 /(1+ np.exp(-Z))\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):    \n",
    "    np.random.seed(1)\n",
    "    W1 = np.random.randn(n_h,n_x) * 0.01\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h) * 0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    # Retrieving each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Forward Propagation to calculate A2 (probabilities)\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    \n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache\n",
    "\n",
    "def compute_cost(A2, Y, parameters):\n",
    "    \n",
    "    m = Y.shape[1] # number of example\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    logprobs = np.dot(np.log(A2), Y.T) + np.dot(np.log(1 - A2), 1-Y.T)\n",
    "    cost = -1/m * np.sum(logprobs)\n",
    "    \n",
    "    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n",
    "    assert(isinstance(cost, float))\n",
    "    return cost\n",
    "\n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "        \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    dZ2 = A2-Y\n",
    "\n",
    "    dW2 = 1/m * np.dot(dZ2,A1.T)\n",
    "    db2 = 1/m * np.sum(dZ2, axis=1, keepdims = True) # sum all cols in a row\n",
    "    \n",
    "    dZ1 = np.multiply( np.dot(W2.T, dZ2)  , (1 - np.power(A1, 2)) ) \n",
    "    dW1 = 1/m * np.dot(dZ1, X.T)\n",
    "    db1 = 1/m * np.sum(dZ1, axis=1, keepdims = True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    W1 = W1 - learning_rate*dW1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    W2 = W2 - learning_rate*dW2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def nn_model(X, Y, n_h, learning_rate, num_iterations = 10000, print_cost=False):\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    n_x = len(X)\n",
    "    n_y = len(Y)\n",
    "    \n",
    "    # Initialize parameters and retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\". \n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "        cost = compute_cost(A2, Y, parameters)\n",
    " \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    " \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        \n",
    "        # Print the cost every 500 iterations\n",
    "        if print_cost and i % 500 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def predict(parameters, X):    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = (A2>=0.5)\n",
    "    return predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693132\n",
      "Cost after iteration 500: 0.549645\n",
      "Cost after iteration 1000: 0.460085\n",
      "Cost after iteration 1500: 0.409217\n",
      "Cost after iteration 2000: 0.379224\n",
      "Cost after iteration 2500: 0.360313\n",
      "Cost after iteration 3000: 0.347661\n",
      "Cost after iteration 3500: 0.338789\n",
      "Cost after iteration 4000: 0.332379\n",
      "Cost after iteration 4500: 0.327697\n",
      "Cost after iteration 5000: 0.323957\n",
      "Cost after iteration 5500: 0.320704\n",
      "Cost after iteration 6000: 0.317831\n",
      "Cost after iteration 6500: 0.315287\n",
      "Cost after iteration 7000: 0.313029\n",
      "Cost after iteration 7500: 0.311019\n",
      "Cost after iteration 8000: 0.309225\n",
      "Cost after iteration 8500: 0.307620\n",
      "Cost after iteration 9000: 0.306177\n",
      "Cost after iteration 9500: 0.304875\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "parameters = nn_model(feature_matrix_train, sentiment_matrix_train, n_h = 8, learning_rate =1.3, num_iterations = 10000, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 3: compare  with your Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El accuracy obtenido usando sklearn.linear_model.SGDClassifier es de: 0.867\n",
    "\n",
    "El accuracy obtenido usando una red neuronal con una capa oculta es;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set\n",
      "Accuracy: 87.115556%\n",
      "\n",
      "Accuracy on validation set\n",
      "Accuracy: 87.340000%\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "print(\"Accuracy on training set\")\n",
    "predictions = predict(parameters, feature_matrix_train)\n",
    "print ('Accuracy: %f' % float((np.dot(sentiment_matrix_train,predictions.T) + np.dot(1-sentiment_matrix_train,1-predictions.T))/float(sentiment_matrix_train.size)*100) + '%')\n",
    "\n",
    "print(\"\\nAccuracy on validation set\")\n",
    "predictions = predict(parameters, feature_matrix_valid)\n",
    "print ('Accuracy: %f' % float((np.dot(sentiment_matrix_valid,predictions.T) + np.dot(1-sentiment_matrix_valid,1-predictions.T))/float(sentiment_matrix_valid.size)*100) + '%') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
