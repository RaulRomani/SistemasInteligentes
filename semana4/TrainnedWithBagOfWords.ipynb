{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "#dealing with operating system , like reading file\n",
    "import os\n",
    "\n",
    "#from nltk.corpus import stopwords\n",
    "from __future__ import division\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a tokenized corpus for a word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('shuffled_movie_data.csv')\n",
    "\n",
    "#SOME CLEANING\n",
    "def remove_punctuation(text):    \n",
    "    return text.translate(None, string.punctuation) \n",
    "\n",
    "# Remove punctuation.\n",
    "df['review_clean'] = df['review'].apply(remove_punctuation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the corpus\n",
    "corpus = df['review_clean'].values.tolist()\n",
    "\n",
    "# Tokenize corpus\n",
    "#tok_corpus = [nltk.word_tokenize(sent.decode('utf-8')) for sent in corpus ]\n",
    "\n",
    "#tokenizastion! saved the trained model here\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "tok_corpus = tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK so I really like Kris Kristofferson and his usual easy going delivery of lines in his movies Age has helped him with his soft spoken low energy style and he will steal a scene effortlessly But Disappearance is his misstep Holy Moly this was a bad movie br br I must give kudos to the cinematography and and the actors including Kris for trying their darndest to make sense from this goofy confusing story None of it made sense and Kris probably didnt understand it either and he was just going through the motions hoping someone would come up to him and tell him what it was all about br br I dont care that everyone on this movie was doing out of love for the project or some such nonsense Ive seen low budget movies that had a plot for goodness sake This had none zilcho nada zippo empty of reason a complete waste of good talent scenery and celluloid br br I rented this piece of garbage for a buck and I want my money back I want my 2 hours back I invested on this Grade F waste of my time Dont watch this movie or waste 1 minute of your valuable time while passing through a room where its playing or even open up the case that is holding the DVD Believe me youll thank me for the advice\n",
      "['OK', 'so', 'I', 'really', 'like', 'Kris', 'Kristofferson', 'and', 'his', 'usual', 'easy', 'going', 'delivery', 'of', 'lines', 'in', 'his', 'movies', 'Age', 'has', 'helped', 'him', 'with', 'his', 'soft', 'spoken', 'low', 'energy', 'style', 'and', 'he', 'will', 'steal', 'a', 'scene', 'effortlessly', 'But', 'Disappearance', 'is', 'his', 'misstep', 'Holy', 'Moly', 'this', 'was', 'a', 'bad', 'movie', 'br', 'br', 'I', 'must', 'give', 'kudos', 'to', 'the', 'cinematography', 'and', 'and', 'the', 'actors', 'including', 'Kris', 'for', 'trying', 'their', 'darndest', 'to', 'make', 'sense', 'from', 'this', 'goofy', 'confusing', 'story', 'None', 'of', 'it', 'made', 'sense', 'and', 'Kris', 'probably', 'didnt', 'understand', 'it', 'either', 'and', 'he', 'was', 'just', 'going', 'through', 'the', 'motions', 'hoping', 'someone', 'would', 'come', 'up', 'to', 'him', 'and', 'tell', 'him', 'what', 'it', 'was', 'all', 'about', 'br', 'br', 'I', 'dont', 'care', 'that', 'everyone', 'on', 'this', 'movie', 'was', 'doing', 'out', 'of', 'love', 'for', 'the', 'project', 'or', 'some', 'such', 'nonsense', 'Ive', 'seen', 'low', 'budget', 'movies', 'that', 'had', 'a', 'plot', 'for', 'goodness', 'sake', 'This', 'had', 'none', 'zilcho', 'nada', 'zippo', 'empty', 'of', 'reason', 'a', 'complete', 'waste', 'of', 'good', 'talent', 'scenery', 'and', 'celluloid', 'br', 'br', 'I', 'rented', 'this', 'piece', 'of', 'garbage', 'for', 'a', 'buck', 'and', 'I', 'want', 'my', 'money', 'back', 'I', 'want', 'my', 'hours', 'back', 'I', 'invested', 'on', 'this', 'Grade', 'F', 'waste', 'of', 'my', 'time', 'Dont', 'watch', 'this', 'movie', 'or', 'waste', 'minute', 'of', 'your', 'valuable', 'time', 'while', 'passing', 'through', 'a', 'room', 'where', 'its', 'playing', 'or', 'even', 'open', 'up', 'the', 'case', 'that', 'is', 'holding', 'the', 'DVD', 'Believe', 'me', 'youll', 'thank', 'me', 'for', 'the', 'advice']\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "print corpus[1]\n",
    "\n",
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \", raw)\n",
    "    words = clean.split()\n",
    "    return words\n",
    "\n",
    "print sentence_to_wordlist(corpus[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a word2vec model (using gensim library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "#step 2 build our model, another one is Glove\n",
    "#define hyperparameters\n",
    "\n",
    "# Dimensionality of the resulting word vectors.\n",
    "#more dimensions mean more traiig them, but more generalized\n",
    "num_features = 300\n",
    "\n",
    "#\n",
    "# Minimum word count threshold.\n",
    "min_word_count = 12\n",
    "\n",
    "# Number of threads to run in parallel.\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "# Context window length.\n",
    "context_size = 7\n",
    "\n",
    "# Downsample setting for frequent words.\n",
    "#rate 0 and 1e-5 \n",
    "#how often to use\n",
    "downsampling = 1e-3\n",
    "\n",
    "# Seed for the RNG, to make the results reproducible.\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews2vec = gensim.models.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Word2Vec vocabulary length:', 61388)\n"
     ]
    }
   ],
   "source": [
    "reviews2vec.build_vocab(tok_corpus)\n",
    "print(\"Word2Vec vocabulary length:\", len(reviews2vec.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28585120"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews2vec.train(tok_corpus, total_examples=reviews2vec.corpus_count, epochs=reviews2vec.iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save and load the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save model\n",
    "if not os.path.exists(\"trained\"):\n",
    "    os.makedirs(\"trained\")\n",
    "reviews2vec.save(os.path.join(\"trained\", \"reviews2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you already have trained a model, you can start executing the lines from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load model\n",
    "reviews2vec = gensim.models.Word2Vec.load(os.path.join(\"trained\", \"reviews2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 1: define new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['herei', 'subpar', 'comparebr', 'thrillerdrama', 'goodfor', 'actuallybr', 'cheesiest', 'suckedbr', 'storyor', 'moviejust', 'stupidest', 'watchin', 'seenand', 'badi', 'badand', 'badi', 'finest', 'lousy', 'weak', 'nominees', 'dreadful', 'awfulbr', 'funniest', 'horrible', 'dullest', 'badand', 'goodand', 'badmouth', 'catastrophically', 'firstthe', 'terriblethe', 'funnyso', 'crappiest', 'greatthe', 'engagingbr', 'exciting', 'nostalgias', 'goodthe', 'strangest', 'nominationsbr', 'finebr', 'skimmed', 'storywriting', 'wasi', 'dreadful', 'terrible', 'goodi', 'substantive', 'apalling', 'badthe', 'terriblebr', 'fantastic', 'whodoneit', 'dreadfulbr', 'greatthe', 'substandard', 'slowest', 'catastrophically', 'baftas', 'fascinating', 'outstanding', 'sillybut', 'terriblethe', 'superb', 'comedymusical', 'terrific', 'horrible', 'badand', 'bestworst', 'nominationbr', 'terrible', 'abit', 'horrible', 'actium', 'stunningbr', 'exceptional', 'awfulbr', 'halfbad', 'badi', 'badand', 'poorthe', 'goodit', 'superlative', 'horrendous', 'perv', 'ohand', 'awsome', 'weirdest', 'awardsbr', 'outstandingbr', 'stupidthe', 'fantastic', 'horrendousbr', 'tas', 'lousy', 'hillarious', 'nitpicky', 'topnotch', 'overemotional', 'marvellous', 'stupidthe', 'amazing', 'lamest', 'kickass', 'candies', 'dumbest', 'suckedbr', 'dreadfulbr', 'kamerling', 'guldbagge', 'terrible', 'terriblebr', 'fledged', 'apalling', 'intriguing', 'stupidthe', 'lousy', 'terriblethe', 'adjl', 'horriblebr']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word_list = ['awful', \"awesome\", \"like\",\"really\", \"good\", \"best\", \"interesting\", \"worst\", \"excellent\", \"poor\", \"awful\", \"terrible\"  ]\n",
    "\n",
    "important_words =[]\n",
    "\n",
    "for word in word_list:    \n",
    "    top_words = reviews2vec.most_similar(word, topn=10)\n",
    "    top_words = [ word[0].encode('ascii','ignore') for word in top_words ]\n",
    "    important_words += top_words\n",
    "    \n",
    "from random import shuffle\n",
    "# shuffle the features\n",
    "shuffle(important_words)\n",
    "print important_words\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'horrible', 0.6686462163925171),\n",
       " (u'terriblebr', 0.6507787704467773),\n",
       " (u'dreadful', 0.6422439217567444),\n",
       " (u'lousy', 0.6419680118560791),\n",
       " (u'awfulbr', 0.641074538230896),\n",
       " (u'terrible', 0.6394296288490295),\n",
       " (u'catastrophically', 0.6370165944099426),\n",
       " (u'badand', 0.6366477012634277),\n",
       " (u'terriblethe', 0.6366315484046936),\n",
       " (u'stupidthe', 0.6365667581558228)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews2vec.most_similar('awful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This will take a while\n",
    "for word in important_words:\n",
    "    df[word] = df['review_clean'].apply(lambda s : s.split().count(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "print len(important_words)\n",
    "\n",
    "#df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 2: implement a MaxEnt classifier, using regularization, according this https://web.stanford.edu/~jurafsky/slp3/7.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split data \n",
    "#90% - train set,\n",
    "#10% - validation set,\n",
    "train_data, validation_data = np.split(df.sample(frac=1), [int(.9*len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 202)\n",
      "(5000, 202)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>atrocious</th>\n",
       "      <th>astounding</th>\n",
       "      <th>sfx</th>\n",
       "      <th>incredible</th>\n",
       "      <th>Wellthe</th>\n",
       "      <th>appalling</th>\n",
       "      <th>BADbr</th>\n",
       "      <th>...</th>\n",
       "      <th>knowall</th>\n",
       "      <th>TERRIBLE</th>\n",
       "      <th>stupidest</th>\n",
       "      <th>welland</th>\n",
       "      <th>Gerrys</th>\n",
       "      <th>intriguing</th>\n",
       "      <th>WEBS</th>\n",
       "      <th>Blevel</th>\n",
       "      <th>lamest</th>\n",
       "      <th>impeccable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24868</th>\n",
       "      <td>A tedious gangster film that leaves you wishin...</td>\n",
       "      <td>0</td>\n",
       "      <td>A tedious gangster film that leaves you wishin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43562</th>\n",
       "      <td>Joseph L. Mankiewicz's Sleuth didn't need a re...</td>\n",
       "      <td>0</td>\n",
       "      <td>Joseph L Mankiewiczs Sleuth didnt need a remak...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13313</th>\n",
       "      <td>Evan Almighty continues the mainstream Bruce A...</td>\n",
       "      <td>0</td>\n",
       "      <td>Evan Almighty continues the mainstream Bruce A...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49159</th>\n",
       "      <td>Look, although we don't like to admit it, we'v...</td>\n",
       "      <td>1</td>\n",
       "      <td>Look although we dont like to admit it weve al...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43120</th>\n",
       "      <td>I have lost count of just how many times I hav...</td>\n",
       "      <td>1</td>\n",
       "      <td>I have lost count of just how many times I hav...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment  \\\n",
       "24868  A tedious gangster film that leaves you wishin...          0   \n",
       "43562  Joseph L. Mankiewicz's Sleuth didn't need a re...          0   \n",
       "13313  Evan Almighty continues the mainstream Bruce A...          0   \n",
       "49159  Look, although we don't like to admit it, we'v...          1   \n",
       "43120  I have lost count of just how many times I hav...          1   \n",
       "\n",
       "                                            review_clean  atrocious  \\\n",
       "24868  A tedious gangster film that leaves you wishin...          0   \n",
       "43562  Joseph L Mankiewiczs Sleuth didnt need a remak...          0   \n",
       "13313  Evan Almighty continues the mainstream Bruce A...          0   \n",
       "49159  Look although we dont like to admit it weve al...          0   \n",
       "43120  I have lost count of just how many times I hav...          0   \n",
       "\n",
       "       astounding  sfx  incredible  Wellthe  appalling  BADbr     ...      \\\n",
       "24868           0    0           0        0          0      0     ...       \n",
       "43562           0    0           0        0          0      0     ...       \n",
       "13313           0    0           0        0          0      0     ...       \n",
       "49159           0    0           0        0          0      0     ...       \n",
       "43120           0    0           0        0          0      0     ...       \n",
       "\n",
       "       knowall  TERRIBLE  stupidest  welland  Gerrys  intriguing  WEBS  \\\n",
       "24868        0         0          0        0       0           0     0   \n",
       "43562        0         0          0        0       0           0     0   \n",
       "13313        0         0          0        0       0           0     0   \n",
       "49159        0         0          0        0       0           0     0   \n",
       "43120        0         0          0        0       0           0     0   \n",
       "\n",
       "       Blevel  lamest  impeccable  \n",
       "24868       0       0           0  \n",
       "43562       0       0           0  \n",
       "13313       0       0           0  \n",
       "49159       0       0           0  \n",
       "43120       0       0           0  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print train_data.shape\n",
    "print validation_data.shape\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_numpy_data(data_frame, output_name, features_name):\n",
    "    data_frame_copy = data_frame.copy()\n",
    "    \n",
    "    features_frame = pd.core.frame.DataFrame()    \n",
    "    for feature_name in features_name:\n",
    "        features_frame[feature_name] = data_frame_copy[feature_name]\n",
    "    feature_matrix = features_frame.values\n",
    "    output_sarray = data_frame[output_name]\n",
    "    output_array = output_sarray.values\n",
    "    return(feature_matrix, output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Passing data to numpy matrix and array\n",
    "feature_matrix_train, sentiment_train = get_numpy_data( train_data, 'sentiment', important_words)\n",
    "feature_matrix_valid, sentiment_valid = get_numpy_data( validation_data, 'sentiment', important_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiment_train = np.reshape(sentiment_train, (-1, 1))\n",
    "sentiment_train = sentiment_train.T\n",
    "feature_matrix_train = feature_matrix_train.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95, 45000)\n",
      "(1, 45000)\n"
     ]
    }
   ],
   "source": [
    "print feature_matrix_train.shape\n",
    "print sentiment_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95, 5000)\n",
      "(1, 5000)\n"
     ]
    }
   ],
   "source": [
    "sentiment_valid = np.reshape(sentiment_valid, (-1, 1))\n",
    "sentiment_valid = sentiment_valid.T\n",
    "feature_matrix_valid = feature_matrix_valid.T\n",
    "\n",
    "print feature_matrix_valid.shape\n",
    "print sentiment_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Helper method for gradient descent algorithm\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \n",
    "    \n",
    "    # Computing P(y_i = +1 | x_i, w) using the link function\n",
    "    #predictions = 1 /(1+ np.exp(-Z))  #predictions in probabilities\n",
    "    \n",
    "    return 1 /(1+ np.exp(-Z))\n",
    "\n",
    "\n",
    "def layer_sizes(X, Y):\n",
    "    n_x = len(X) # size of input layer\n",
    "    n_h = 4\n",
    "    n_y = len(Y) # size of output layer\n",
    "    return (n_x, n_h, n_y)\n",
    "\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \n",
    "    np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random.\n",
    "\n",
    "    W1 = np.random.randn(n_h,n_x) * 0.01\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h) * 0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    \n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache\n",
    "\n",
    "\n",
    "def compute_cost(A2, Y, parameters):\n",
    "    \n",
    "    m = Y.shape[1] # number of example\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    logprobs = np.dot(np.log(A2), Y.T) + np.dot(np.log(1 - A2), 1-Y.T)\n",
    "    cost = -1/m * np.sum(logprobs)\n",
    "    \n",
    "    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n",
    "    assert(isinstance(cost, float))\n",
    "    return cost\n",
    "\n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "        \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    dZ2 = A2-Y\n",
    "\n",
    "    dW2 = 1/m * np.dot(dZ2,A1.T)\n",
    "    db2 = 1/m * np.sum(dZ2, axis=1, keepdims = True) # sum all cols in a row\n",
    "    \n",
    "    dZ1 = np.multiply( np.dot(W2.T, dZ2)  , (1 - np.power(A1, 2)) ) \n",
    "    dW1 = 1/m * np.dot(dZ1, X.T)\n",
    "    db1 = 1/m * np.sum(dZ1, axis=1, keepdims = True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    W1 = W1 - learning_rate*dW1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    W2 = W2 - learning_rate*dW2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    \n",
    "    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\". \n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "        cost = compute_cost(A2, Y, parameters)\n",
    " \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    " \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "        \n",
    "        \n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def predict(parameters, X):\n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = (A2>=0.5)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693159\n"
     ]
    }
   ],
   "source": [
    "parameters = nn_model(feature_matrix_train, sentiment_train, n_h = 8, num_iterations = 1000, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69%\n",
      "Accuracy: 69%\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "predictions = predict(parameters, feature_matrix_train)\n",
    "print ('Accuracy: %d' % float((np.dot(sentiment_train,predictions.T) + np.dot(1-sentiment_train,1-predictions.T))/float(sentiment_train.size)*100) + '%')\n",
    "\n",
    "\n",
    "predictions = predict(parameters, feature_matrix_valid)\n",
    "print ('Accuracy: %d' % float((np.dot(sentiment_valid,predictions.T) + np.dot(1-sentiment_valid,1-predictions.T))/float(sentiment_valid.size)*100) + '%') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
